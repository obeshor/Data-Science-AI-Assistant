{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 64148,
          "databundleVersionId": 7669720,
          "sourceType": "competition"
        },
        {
          "sourceId": 168178971,
          "sourceType": "kernelVersion"
        },
        {
          "sourceId": 10417,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 8385
        },
        {
          "sourceId": 11264,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 8318
        }
      ],
      "dockerImageVersionId": 30665,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/obeshor/Data-Science-AI-Assistant/blob/main/DS_AI_Assistant_with_Gemma_2b_it.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'data-assistants-with-gemma:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F64148%2F7669720%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240326%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240326T152435Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5fa46fc97da7c495ad8d832f180d0853b41560da4c7b85f37a7bb36dd89d0df14a9fcb292b0e9d418abaef7e38c86f42ac6edb45df9d6b50c33483976f4b7533b7190266a7ec5e9a5545bf267e24db7099bc5a71cbddc75d664c26d63ce705a5b3415af8b7c45e13a13df1dd13ea5bce7552f0dc81195601f46e51a29cc6d8c99b014dba5e35c45fe66498ab97d9fc29644085dccdb2cc5ac23c9aee65d61ce9cd28bd13c46d8c4533b2119bf1995cbc0dd1ffdd52fc64d805e7f4b21466ba32eb70af123f2cc8aafb34e33eb325e7fb34c5c7241739524cd481d370d4d2b9d7fd572b634f656f9f8c96879ae8ea303cd2cc21f93be90b3c1b6a474da17a208e,gemma/gemmacpp/2b-it-sfp/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F8385%2F10417%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240326%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240326T152435Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D51ef01fcab68c8274ea3c336660307a50f9d92c432826718098a6fe5310336da71ae4159e55c8b642c5345c61f16ffc7720ad8ce0c9e866eb1e7c812b6f51d45728b22abffc537ccdd64a784e5f73d6a190f20f295ec556cfab1473c1627c5f91c95ff3466cfd411e5eb844e94121009c1e9bf9a22ecffecdfee5b48c8b4f0435662e0be2b31042d0d535f806a66e9bc09ad033ff908d1c89e63fe2e7b991e89a8c90a953828f84ff017c675243693e61c06ab7b1934ec352dc3a43b29c301fa601b6393addd0f051e7050822664a86d0870f542e632c835a2697c7fc116a802f539e3ab0541c418ddaea9b0ff09f65918715f23ebdf7d4463596d3f872f12d6,gemma/transformers/2b-it/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F8318%2F11264%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240326%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240326T152435Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4750c2f40739247b499ad7572a5ee2b8c02c3182d6cae21577e72245c58749230646433305a4f9cfc305fdf5191fd85b1258ad5324ccbf2d1141a4c90cbb2aad3e5eb46744c7e677f3a4b65b08fef02464328a1462650ba850ccc77be6049835e5f9b2b2d067deaa4d6abf406859b02e642df8d84fb192aa6c83097916b43cb0ae89fb0beeb700a085b30abff67adef52cb528ac28934377e142464df66f69fab571952ad7905d9f81313b40b6a6bb1ba3d475185094098620bb71470b5b3c00e7021387eefb4982c720efe85776b5323e28c4c7ec6bf2561c1dfb49be1bee51b674a0e8a6a0c480f834ce1ae07e926091690d21d343e1a76028a3c9296b76df'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "e1etf3kIXRCO"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Science AI Assistant with Gemma 2b-it"
      ],
      "metadata": {
        "id": "8cgkuLoEXRCS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the Kaggle competition **\"Google â€“ AI Assistants for Data Tasks with Gemma,\"** I've prepared this AI Assistant that can accomplish the task of **\"Explaining or teaching basic data science concepts,\"** as required by the competition.\n",
        "\n",
        "This project is also an occasion to explain how a basic **retrieval-augmented generation (RAG)** system works, by showing the role of the data, which constitutes the backbone of the system, the function of embeddings and distance measures, how to retrieve relevant information for the task of answering a question, and how to process such information by first using a distillation prompt and then assembling the answer required by the user in a meaningful and useful way.\n",
        "\n",
        "In this project, the lion's share is done by **Gemma**, the state-of-the-art open LLM model released by Google, in its <U>2b-it implementation, the smallest in terms of parameters</U>. Gemma is not the only Google technology presented in the project because I also make use of **ScaNN** ([ScaNN Github repository](https://github.com/google-research/google-research/tree/master/scann)) for recalling the information. Apart from Gemma, ScaNN, and HuggingFace packages for transformers and embeddings, there are no ready-made solutions such as vector stores or RAG packages. You can actually see how everything works under the hood, and if you like it, reuse it for your own projects.\n",
        "\n"
      ],
      "metadata": {
        "id": "7JnRBloDXRCT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What is a RAG and how it can help to explain or teach basic data science concepts"
      ],
      "metadata": {
        "id": "lmCD2mzxXRCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **Retrieval-Augmented Generation (RAG)** is a solution that improves text generation of a large language model by integrating its answers using some kind of external knowledge retrieval.\n",
        "\n",
        "Hence, it combines a **retriever** to fetch relevant information and a **generator** to produce accurate responses based on this retrieved knowledge. Basically, it is just like first doing a search engine query (the retriever), getting the best answers, and then asking a large language model such as **Gemma** or **Gemini** to process the information (generator) to answer an initial question."
      ],
      "metadata": {
        "id": "UX6nI4zWXRCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![High-level RAG architecture](https://raw.githubusercontent.com/lmassaron/useful_stuff/main/High-Level%20RAG%20Architecture_rev2.jpg)"
      ],
      "metadata": {
        "id": "MPIvH1lIXRCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Such an approach ensures AI models have access to up-to-date and relevant facts, improving the quality and reliability of their generated text, especially in tasks like question-answering where factual accuracy is crucial and LLMs are infamous for sometimes coming up with made-up information (hallucinations).\n",
        "\n",
        "In this case, **Google Gemma** seems already quite apt at answering basic questions about data science, but the idea is to further improve its competencies by providing it reliable information about AI, statistics, machine learning, and data science in general."
      ],
      "metadata": {
        "id": "jZeGJoFyXRCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Setting up the necessary stuff"
      ],
      "metadata": {
        "id": "2pPOqmSgXRCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the first cell of this notebook, some key packages for the project are installed or updated to the latest version:\n",
        "\n",
        "1. The first command installs or upgrades the **torch** package quietly, specifying version compatibility for CUDA 11.7 from the PyTorch repository.\n",
        "2. The second command installs or upgrades the **transformers** package to version **4.38.2**, a popular library for natural language processing tasks.\n",
        "3. The third command installs the **accelerate** package, which is used for optimizing machine learning training pipelines.\n",
        "4. The fourth command installs the **bitsandbytes** package from the specified index URL, potentially a custom or private package repository.\n",
        "5. The fifth command installs or upgrades the **sentence_transformers** package, known for providing pre-trained models for sentence embeddings.\n",
        "6. The sixth command installs or upgrades the **scann** package, likely used for approximate nearest neighbor search implementations.\n",
        "7. The seventh command installs or upgrades the **wikipedia-api** package, which provides an interface to interact with Wikipedia data programmatically.\n"
      ],
      "metadata": {
        "id": "iRcC9XXOXRCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U torch --index-url https://download.pytorch.org/whl/cu117\n",
        "!pip install -q -U transformers==\"4.38.2\"\n",
        "!pip install -q accelerate\n",
        "!pip install -q -i https://pypi.org/simple/ bitsandbytes\n",
        "!pip install -q -U sentence_transformers\n",
        "!pip install -q -U scann\n",
        "!pip install -q -U wikipedia-api"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T16:43:43.311434Z",
          "iopub.execute_input": "2024-03-25T16:43:43.311705Z",
          "iopub.status.idle": "2024-03-25T16:46:32.08604Z",
          "shell.execute_reply.started": "2024-03-25T16:43:43.311681Z",
          "shell.execute_reply": "2024-03-25T16:46:32.084942Z"
        },
        "trusted": true,
        "id": "aiWjiyKbXRCW",
        "outputId": "144014cc-0132-4d71-bbb6-0509cf170f89"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-nlp 0.8.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\nsqlalchemy 2.0.25 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\nalbumentations 1.4.0 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\nfastapi 0.108.0 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\ngcsfs 2023.12.2.post1 requires fsspec==2023.12.2, but you have fsspec 2024.2.0 which is incompatible.\njupyterlab 4.1.2 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npydantic 2.5.3 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.\npydantic-core 2.14.6 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\npylibraft 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\nrmm 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow-decision-forests 1.8.1 requires tensorflow~=2.15.0, but you have tensorflow 2.13.1 which is incompatible.\ntensorflow-serving-api 2.14.1 requires tensorflow<3,>=2.14.1, but you have tensorflow 2.13.1 which is incompatible.\ntensorflow-text 2.15.0 requires tensorflow<2.16,>=2.15.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.13.1 which is incompatible.\ntensorstore 0.1.53 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\ntypeguard 4.1.5 requires typing-extensions>=4.7.0; python_version < \"3.12\", but you have typing-extensions 4.5.0 which is incompatible.\nwoodwork 0.28.0 requires numpy<2.0.0,>=1.25.0, but you have numpy 1.24.3 which is incompatible.\nxarray 2024.2.0 requires packaging>=22, but you have packaging 21.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next cell, the code uses the **os** module to set some environment variables. The first line sets the **CUDA_VISIBLE_DEVICES** variable to \"0,\" which instructs CUDA-enabled applications to use only the GPU with index 0 for computation, useful for managing GPU resources in multi-GPU systems. The second line sets **TOKENIZERS_PARALLELISM** to \"false,\" disabling parallelism in the Hugging Face Tokenizers library, potentially useful for troubleshooting or ensuring single-threaded execution. These environment variable configurations help control GPU usage and tokenizer behavior within the Python environment where this code is executed."
      ],
      "metadata": {
        "id": "Lt_1faQQXRCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T16:47:46.435232Z",
          "iopub.execute_input": "2024-03-25T16:47:46.435802Z",
          "iopub.status.idle": "2024-03-25T16:47:46.440061Z",
          "shell.execute_reply.started": "2024-03-25T16:47:46.435766Z",
          "shell.execute_reply": "2024-03-25T16:47:46.439138Z"
        },
        "trusted": true,
        "id": "THgvTcO0XRCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moreover, since warnings may occur when using new versions of Python packages (aligning versions is often a task in itself), the following cell imports the **warnings** package and suppresses warnings during this session."
      ],
      "metadata": {
        "id": "A6_zaUdXXRCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T16:47:51.687128Z",
          "iopub.execute_input": "2024-03-25T16:47:51.687933Z",
          "iopub.status.idle": "2024-03-25T16:47:51.691898Z",
          "shell.execute_reply.started": "2024-03-25T16:47:51.687897Z",
          "shell.execute_reply": "2024-03-25T16:47:51.690743Z"
        },
        "trusted": true,
        "id": "XKFJRJsSXRCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next cell, the notebook loads Python libraries and modules for natural language processing tasks. It also includes libraries like **re** for regular expressions, **numpy** and **pandas** for data manipulation, **tqdm** for progress bars, **scann** for approximate nearest neighbor search, and **wikipediaapi** for accessing Wikipedia content (yes, we are going to use Wikipedia as a knowledge base).\n"
      ],
      "metadata": {
        "id": "T9yHyRuxXRCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import scann\n",
        "import wikipediaapi\n",
        "\n",
        "import torch\n",
        "\n",
        "import transformers\n",
        "from transformers import (AutoModelForCausalLM,\n",
        "                          AutoTokenizer,\n",
        "                          BitsAndBytesConfig,\n",
        "                         )\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import bitsandbytes as bnb"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T16:47:57.191889Z",
          "iopub.execute_input": "2024-03-25T16:47:57.192262Z",
          "iopub.status.idle": "2024-03-25T16:48:06.779346Z",
          "shell.execute_reply.started": "2024-03-25T16:47:57.192233Z",
          "shell.execute_reply": "2024-03-25T16:48:06.77855Z"
        },
        "trusted": true,
        "id": "Me56UVkMXRCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Proceeding by building blocks"
      ],
      "metadata": {
        "id": "zKg6eZiqXRCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before proceeding with the notebook, it is necessary to spend a word about how I will proceed in building the solution in a way that can be clear, easily explainable, and both reusable as well as hackable."
      ],
      "metadata": {
        "id": "KFJD-ePAXRCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The AI assistant will simply be a class containing all you need for it to work and with methods for changing some settings (such as the temperature, which corresponds to its creativity, or the impersonated role, which influences how it responds) and for asking questions.\n",
        "\n",
        "All the internal functions, however, are external, hence they are easier to present as stand-alone code snippets, easily reusable for different purposes or projects, and easily upgradable or hackable. Because as you change an external function, you immediately change the behavior of the class, without having to reinstantiate it again (it actually takes some time to re-index all the knowledge base, which may prevent some fast experimentation).\n"
      ],
      "metadata": {
        "id": "VGzbO_b9XRCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, as a first piece of code, the next cell presents a function that returns the device where to map the model and the data when working with the PyTorch library (used by the HF packages). It works with a **CPU-based** computer, a **GPU** one, and with a **macOS with MPS**."
      ],
      "metadata": {
        "id": "AfEKfykIXRCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def define_device():\n",
        "    \"\"\"Define the device to be used by PyTorch\"\"\"\n",
        "\n",
        "    # Get the PyTorch version\n",
        "    torch_version = torch.__version__\n",
        "\n",
        "    # Print the PyTorch version\n",
        "    print(f\"PyTorch version: {torch_version}\", end=\" -- \")\n",
        "\n",
        "    # Check if MPS (Multi-Process Service) device is available on MacOS\n",
        "    if torch.backends.mps.is_available():\n",
        "        # If MPS is available, print a message indicating its usage\n",
        "        print(\"using MPS device on MacOS\")\n",
        "        # Define the device as MPS\n",
        "        defined_device = torch.device(\"mps\")\n",
        "    else:\n",
        "        # If MPS is not available, determine the device based on GPU availability\n",
        "        defined_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        # Print a message indicating the selected device\n",
        "        print(f\"using {defined_device}\")\n",
        "\n",
        "    # Return the defined device\n",
        "    return defined_device\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T16:49:25.676181Z",
          "iopub.execute_input": "2024-03-25T16:49:25.677152Z",
          "iopub.status.idle": "2024-03-25T16:49:25.683942Z",
          "shell.execute_reply.started": "2024-03-25T16:49:25.677101Z",
          "shell.execute_reply": "2024-03-25T16:49:25.683033Z"
        },
        "trusted": true,
        "id": "P7GD-bUmXRCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cells, instead, present two functions designed to operate using the **SentenceTransformers** package ([the package home page](https://www.sbert.net/index.html)), that can operate with lists of text and map them into embeddings.\n"
      ],
      "metadata": {
        "id": "f7j_qH0iXRCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embeddings, such as those processed by packages like **SentenceTransformers**, are representations of text or sentences in a numerical form that capture their semantic meaning. These embeddings are created by transforming words or sentences into high-dimensional vectors, where similar vectors represent similar meanings."
      ],
      "metadata": {
        "id": "-PiYfyZwXRCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of **SentenceTransformers**, these embeddings are generated using models like BERT or XLNet that have been fine-tuned to produce meaningful sentence representations. These embeddings can be used for various tasks like clustering, semantic textual similarity, and information retrieval (in our project we actually need a retrieval function) by comparing the vectors using metrics like cosine similarity."
      ],
      "metadata": {
        "id": "rhL2x1i_XRCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(text, embedding_model):\n",
        "    \"\"\"Get embeddings for a given text using the provided embedding model\"\"\"\n",
        "\n",
        "    # Encode the text to obtain embeddings using the provided embedding model\n",
        "    embedding = embedding_model.encode(text, show_progress_bar=False)\n",
        "\n",
        "    # Convert the embeddings to a list of floats and return\n",
        "    return embedding.tolist()\n",
        "\n",
        "def map2embeddings(data, embedding_model):\n",
        "    \"\"\"Map a list of texts to their embeddings using the provided embedding model\"\"\"\n",
        "\n",
        "    # Initialize an empty list to store embeddings\n",
        "    embeddings = []\n",
        "\n",
        "    # Iterate over each text in the input data list\n",
        "    no_texts = len(data)\n",
        "    print(f\"Mapping {no_texts} pieces of information\")\n",
        "    for i in tqdm(range(no_texts)):\n",
        "        # Get embeddings for the current text using the provided embedding model\n",
        "        embeddings.append(get_embedding(data[i], embedding_model))\n",
        "\n",
        "    # Return the list of embeddings\n",
        "    return embeddings"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T16:50:32.755848Z",
          "iopub.execute_input": "2024-03-25T16:50:32.756394Z",
          "iopub.status.idle": "2024-03-25T16:50:32.764003Z",
          "shell.execute_reply.started": "2024-03-25T16:50:32.75636Z",
          "shell.execute_reply": "2024-03-25T16:50:32.763049Z"
        },
        "trusted": true,
        "id": "JV6JreMLXRCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell contains a simple function capable of removing artifacts such as tokens, double asterisks, or spaces which sometimes appear in outputs from large language models.\n"
      ],
      "metadata": {
        "id": "lftegQl6XRCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(txt, EOS_TOKEN):\n",
        "    \"\"\"Clean text by removing specific tokens and redundant spaces\"\"\"\n",
        "    txt = (txt\n",
        "           .replace(EOS_TOKEN, \"\") # Replace the end-of-sentence token with an empty string\n",
        "           .replace(\"**\", \"\")      # Replace double asterisks with an empty string\n",
        "           .replace(\"<pad>\", \"\")   # Replace \"<pad>\" with an empty string\n",
        "           .replace(\"  \", \" \")     # Replace double spaces with single spaces\n",
        "          ).strip()                # Strip leading and trailing spaces from the text\n",
        "    return txt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T16:51:30.472295Z",
          "iopub.execute_input": "2024-03-25T16:51:30.472646Z",
          "iopub.status.idle": "2024-03-25T16:51:30.478434Z",
          "shell.execute_reply.started": "2024-03-25T16:51:30.472619Z",
          "shell.execute_reply": "2024-03-25T16:51:30.477393Z"
        },
        "trusted": true,
        "id": "WdxoRcZ7XRCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function, instead, simply adds an indefinite article to a role name, something useful to make a prompt nicer and easier to read.\n"
      ],
      "metadata": {
        "id": "HuzRRRHIXRCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_indefinite_article(role_name):\n",
        "    \"\"\"Check if a role name has a determinative adjective before it, and if not, add the correct one\"\"\"\n",
        "\n",
        "    # Check if the first word is a determinative adjective\n",
        "    determinative_adjectives = [\"a\", \"an\", \"the\"]\n",
        "    words = role_name.split()\n",
        "    if words[0].lower() not in determinative_adjectives:\n",
        "        # Use \"a\" or \"an\" based on the first letter of the role name\n",
        "        determinative_adjective = \"an\" if words[0][0].lower() in \"aeiou\" else \"a\"\n",
        "        role_name = f\"{determinative_adjective} {role_name}\"\n",
        "\n",
        "    return role_name"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T16:51:51.084848Z",
          "iopub.execute_input": "2024-03-25T16:51:51.085246Z",
          "iopub.status.idle": "2024-03-25T16:51:51.091231Z",
          "shell.execute_reply.started": "2024-03-25T16:51:51.085215Z",
          "shell.execute_reply": "2024-03-25T16:51:51.090313Z"
        },
        "trusted": true,
        "id": "0EjsHOYBXRCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the previous functions, mostly devoted to processing text for better readability, the next class helps first to load and initialize Gemma by quantizing it to 4-bit, reducing its memory footprint and allowing for faster responses, and then to generate text from it. <u>**Gemma** is the core of our generative functions</u>, making it a crucial element for processing information and returning it to the user in the most usable and useful form.\n"
      ],
      "metadata": {
        "id": "qXm87HfyXRCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `GemmaHF` class serves as a wrapper for the Transformers implementation of Gemma. Upon initialization, it sets up the model and tokenizer using the specified model name and a maximum sequence length for the tokenizer.\n",
        "\n",
        "In short, the method `initialize_model` is designed to set up a 4-bit quantized causal language model (LLM) and tokenizer and configure them. It begins by defining the data type for computation as `float16`. Then, it creates a configuration for quantization using the `BitsAndBytesConfig` class with settings for 4-bit quantization. The function loads a pre-trained model (Gemma 2b-it in the project, but you can try also the 7b version) with the specified quantization configuration. It also loads a tokenizer with the selected device mapping and maximum sequence length settings. Finally, the method returns the initialized model and tokenizer, ready for use by our AI assistant.\n",
        "\n",
        "Finally, its `generate_text` method takes a prompt as input and generates a text using the instantiated tokenizer and model, allowing for customization of parameters such as maximum new tokens and temperature for sampling. Under the hood, it encodes the prompt, generates text based on it, decodes the output into text, and returns a list of generated text results.\n"
      ],
      "metadata": {
        "id": "ezqLvWdyXRCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GemmaHF():\n",
        "    \"\"\"Wrapper for the Transformers implementation of Gemma\"\"\"\n",
        "\n",
        "    def __init__(self, model_name, max_seq_length=2048):\n",
        "        self.model_name = model_name\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "        # Initialize the model and tokenizer\n",
        "        print(\"\\nInitializing model:\")\n",
        "        self.device = define_device()\n",
        "        self.model, self.tokenizer = self.initialize_model(self.model_name, self.device, self.max_seq_length)\n",
        "\n",
        "    def initialize_model(self, model_name, device, max_seq_length):\n",
        "        \"\"\"Initialize a 4-bit quantized causal language model (LLM) and tokenizer with specified settings\"\"\"\n",
        "\n",
        "        # Define the data type for computation\n",
        "        compute_dtype = getattr(torch, \"float16\")\n",
        "\n",
        "        # Define the configuration for quantization\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=compute_dtype,\n",
        "        )\n",
        "\n",
        "        # Load the pre-trained model with quantization configuration\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=device,\n",
        "            quantization_config=bnb_config,\n",
        "        )\n",
        "\n",
        "        # Load the tokenizer with specified device and max_seq_length\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=device,\n",
        "            max_seq_length=max_seq_length\n",
        "        )\n",
        "\n",
        "        # Return the initialized model and tokenizer\n",
        "        return model, tokenizer\n",
        "\n",
        "    def generate_text(self, prompt, max_new_tokens=2048, temperature=0.0):\n",
        "        \"\"\"Generate text using the instantiated tokenizer and model with specified settings\"\"\"\n",
        "\n",
        "        # Encode the prompt and convert to PyTorch tensor\n",
        "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\", padding=True).to(self.device)\n",
        "\n",
        "        # Determine if sampling should be performed based on temperature\n",
        "        do_sample = True if temperature > 0 else False\n",
        "\n",
        "        # Generate text based on the input prompt\n",
        "        outputs = self.model.generate(**input_ids,\n",
        "                                      max_new_tokens=max_new_tokens,\n",
        "                                      do_sample=do_sample,\n",
        "                                      temperature=temperature\n",
        "                                     )\n",
        "\n",
        "        # Decode the generated output into text\n",
        "        results = [self.tokenizer.decode(output) for output in outputs]\n",
        "\n",
        "        # Return the list of generated text results\n",
        "        return results"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T16:53:35.317456Z",
          "iopub.execute_input": "2024-03-25T16:53:35.318098Z",
          "iopub.status.idle": "2024-03-25T16:53:35.328833Z",
          "shell.execute_reply.started": "2024-03-25T16:53:35.318064Z",
          "shell.execute_reply": "2024-03-25T16:53:35.327891Z"
        },
        "trusted": true,
        "id": "DVLNLJPqXRCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here we arrive at the core of the generative function (before we just initialized the generative engine, Gemma)."
      ],
      "metadata": {
        "id": "IUGwDMdZXRCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `generate_summary_and_answer` function, generates an answer for a given question using context from a dataset. It embeds the input question (using the `get_embedding` function we previously saw), finds similar contexts in the dataset, extracts relevant context based on similarity indices, generates prompts for summarizing the context and providing an answer, generates summaries and answers using the a generative method from a \"model\" class, which can be a wrapper class containing Gemma implementations based on HF Transformers, Keras, Gemma C++ or any other available. Afterwards, the function cleans the generated summary and answer, and returns the cleaned answer for further processing. This function works as a sequence of steps in order to generate informative responses starting from an input question and some knowledge base data previously provided.\n"
      ],
      "metadata": {
        "id": "mmoqgcr5XRCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two-step execution processing the information retrieved from the knowledge base is necessary because extraction based on embedded vectors sometimes returns irrelevant information. It is a problem based on the fact that embeddings are a mapping that has many facets (they are high-dimensional themselves) and that distance measures, and methods for finding what documents or text are most similar to your question, are often approximate for performance reasons resulting sometimes in unexpected retrieved results. First summarizing relevant information, a task that Gemma can execute with prowess, helps in having a shorter, more compact, and surely more relevant context to provide to the further processing by Gemma, which consists of writing an answer to your question.\n"
      ],
      "metadata": {
        "id": "nmZQ4oc9XRCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this process, temperature, the level of creativity, and the role may result in different answers and also different answering styles. I decided to rely on the \"expert data scientist\" role, but you may decide for the \"ELI5 divulgator\" or the \"verbose scholarly narrator\" (at your own risk XD).\n"
      ],
      "metadata": {
        "id": "up0THZd3XRCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, notice the part of the generative prompt that says: \"If the context doesn't provide any relevant information, answer with <I couldn't find a good match in my knowledge base for your query, hence I answer based on my own knowledge>\". This is partly to prevent the assistant from losing its usefulness and to alert the user regarding the assistant providing peculiar answers when the question is off-topic, too difficult, or lacks sufficient information.\n"
      ],
      "metadata": {
        "id": "bAkewRAdXRCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary_and_answer(question, data, searcher, embedding_model, model,\n",
        "                                max_new_tokens=2048, temperature=0.4, role=\"expert\"):\n",
        "    \"\"\"Generate an answer for a given question using context from a dataset\"\"\"\n",
        "\n",
        "    # Embed the input question using the provided embedding model\n",
        "    embeded_question = np.array(get_embedding(question, embedding_model)).reshape(1, -1)\n",
        "\n",
        "    # Find similar contexts in the dataset based on the embedded question\n",
        "    neighbors, distances = searcher.search_batched(embeded_question)\n",
        "\n",
        "    # Extract context from the dataset based on the indices of similar contexts\n",
        "    context = \" \".join([data[pos] for pos in np.ravel(neighbors)])\n",
        "\n",
        "    # Get the end-of-sentence token from the tokenizer\n",
        "    try:\n",
        "        EOS_TOKEN = model.tokenizer.eos_token\n",
        "    except:\n",
        "        EOS_TOKEN = \"<eos>\"\n",
        "\n",
        "    # Add a determinative adjective to the role\n",
        "    role = add_indefinite_article(role)\n",
        "\n",
        "    # Generate a prompt for summarizing the context\n",
        "    prompt = f\"\"\"\n",
        "             Summarize this context: \"{context}\" in order to answer the question \"{question}\" as {role}\\\n",
        "             SUMMARY:\n",
        "             \"\"\".strip() + EOS_TOKEN\n",
        "\n",
        "    # Generate a summary based on the prompt\n",
        "    results = model.generate_text(prompt, max_new_tokens, temperature)\n",
        "\n",
        "    # Clean the generated summary\n",
        "    summary = clean_text(results[0].split(\"SUMMARY:\")[-1], EOS_TOKEN)\n",
        "\n",
        "    # Generate a prompt for providing an answer\n",
        "    prompt = f\"\"\"\n",
        "             Here is the context: {summary}\n",
        "             Using the relevant information from the context\n",
        "             and integrating it with your knowledge,\n",
        "             provide an answer as {role} to the question: {question}.\n",
        "             If the context doesn't provide\n",
        "             any relevant information answer with\n",
        "             [I couldn't find a good match in my\n",
        "             knowledge base for your question,\n",
        "             hence I answer based on my own knowledge] \\\n",
        "             ANSWER:\n",
        "             \"\"\".strip() + EOS_TOKEN\n",
        "\n",
        "    # Generate an answer based on the prompt\n",
        "    results = model.generate_text(prompt, max_new_tokens, temperature)\n",
        "\n",
        "    # Clean the generated answer\n",
        "    answer = clean_text(results[0].split(\"ANSWER:\")[-1], EOS_TOKEN)\n",
        "\n",
        "    # Return the cleaned answer\n",
        "    return answer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T16:56:49.972838Z",
          "iopub.execute_input": "2024-03-25T16:56:49.973554Z",
          "iopub.status.idle": "2024-03-25T16:56:49.983473Z",
          "shell.execute_reply.started": "2024-03-25T16:56:49.973519Z",
          "shell.execute_reply": "2024-03-25T16:56:49.982485Z"
        },
        "trusted": true,
        "id": "59vN7SziXRCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Wrapping up everything"
      ],
      "metadata": {
        "id": "nByqHdAFXRCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, the next cell wraps all the functions into an `AIAssistant` class.\n"
      ],
      "metadata": {
        "id": "VKWqzbYEXRCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `AIAssistant` class impersonates an AI assistant that interacts with users by providing answers based on a given knowledge base (basically a list of texts containing the knowledge).\n",
        "\n",
        "Upon initialization, the class loads an embedding model, indexes the knowledge base for efficient search, initializes a language model and tokenizer, and builds a searcher for similarity search using the SCANN library. The class includes functions to query the knowledge base, adjust the assistant's temperature (creativity), and define its answering style.\n",
        "\n",
        "- The `query` function generates and prints an answer to a user query by utilizing the `generate_summary_and_answer` function.\n",
        "- The `set_temperature` function allows adjusting the assistant's creativity level, while the `set_role` function defines the answering style of the AI assistant.\n",
        "\n",
        "This class wraps all together the functionality of an AI assistant that makes good use of embeddings, powerful language models such as Gemma, and similarity search to provide informative responses to user queries based on a predefined knowledge base.\n"
      ],
      "metadata": {
        "id": "jsZCWeNXXRCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few notes about ScaNN. ScaNN is a library developed by Google Research that offers efficient and scalable nearest neighbor search capabilities. It provides advantages over other solutions by utilizing techniques like quantization and Anisotropic Hashing, which enhance search performance.\n",
        "\n",
        "Anisotropic Hashing is a method used in hashing techniques for multimodal retrieval that involves learning projection functions to produce dimensions with varying lengths or scales. This flexibility in scaling can be beneficial for capturing complex relationships and structures in high-dimensional data, offering improved retrieval performance in scenarios where isotropic methods may not be as effective. You can read everything about this method in the paper:\n",
        "\n",
        "Guo, Ruiqi, et al. \"Accelerating large-scale inference with anisotropic vector quantization.\" International Conference on Machine Learning. PMLR, 2020. ([Paper Link](https://arxiv.org/abs/1908.10396))\n",
        "\n",
        "or by browsing the code repository at [https://github.com/google-research/google-research/tree/master/scann](https://github.com/google-research/google-research/tree/master/scann)\n",
        "\n",
        "What is interesting to note is that in my solution I do not use the cosine distance but simply the dot product as suggested by this paper:\n",
        "\n",
        "Steck, Harald, Chaitanya Ekanadham, and Nathan Kallus. \"Is Cosine-Similarity of Embeddings Really About Similarity?.\" arXiv preprint arXiv:2403.05440 (2024). ([Paper Link](https://arxiv.org/html/2403.05440v1))\n",
        "\n",
        "And it works pretty well!\n"
      ],
      "metadata": {
        "id": "34r83wBSXRCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class AIAssistant():\n",
        "    \"\"\"An AI assistant that interacts with users by providing answers based on a provided knowledge base\"\"\"\n",
        "\n",
        "    def __init__(self, gemma_model, embeddings_name=\"thenlper/gte-large\", temperature=0.4, role=\"expert\"):\n",
        "        \"\"\"Initialize the AI assistant.\"\"\"\n",
        "        # Initialize attributes\n",
        "        self.embeddings_name = embeddings_name\n",
        "        self.knowledge_base = []\n",
        "        self.temperature = temperature\n",
        "        self.role = role\n",
        "\n",
        "        # Initialize Gemma model (it can be transformer-based or any other)\n",
        "        self.gemma_model = gemma_model\n",
        "\n",
        "        # Load the embedding model\n",
        "        self.embedding_model = SentenceTransformer(self.embeddings_name)\n",
        "\n",
        "    def store_knowledge_base(self, knowledge_base):\n",
        "        \"\"\"Store the knowledge base\"\"\"\n",
        "        self.knowledge_base=knowledge_base\n",
        "\n",
        "    def learn_knowledge_base(self, knowledge_base):\n",
        "        \"\"\"Store and index the knowledge based to be used by the assistant\"\"\"\n",
        "        # Storing the knowledge base\n",
        "        self.store_knowledge_base(knowledge_base)\n",
        "\n",
        "        # Load and index the knowledge base\n",
        "        print(\"Indexing and mapping the knowledge base:\")\n",
        "        embeddings = map2embeddings(self.knowledge_base, self.embedding_model)\n",
        "        self.embeddings = np.array(embeddings).astype(np.float32)\n",
        "\n",
        "        # Instantiate the searcher for similarity search\n",
        "        self.index_embeddings()\n",
        "\n",
        "    def index_embeddings(self):\n",
        "        \"\"\"Index the embeddings using ScaNN \"\"\"\n",
        "        self.searcher = (scann.scann_ops_pybind.builder(db=self.embeddings, num_neighbors=10, distance_measure=\"dot_product\")\n",
        "                 .tree(num_leaves=min(self.embeddings.shape[0] // 2, 1000),\n",
        "                       num_leaves_to_search=100,\n",
        "                       training_sample_size=self.embeddings.shape[0])\n",
        "                 .score_ah(2, anisotropic_quantization_threshold=0.2)\n",
        "                 .reorder(100)\n",
        "                 .build()\n",
        "           )\n",
        "\n",
        "    def query(self, query):\n",
        "        \"\"\"Query the knowledge base of the AI assistant.\"\"\"\n",
        "        # Generate and print an answer to the query\n",
        "        answer = generate_summary_and_answer(query,\n",
        "                                             self.knowledge_base,\n",
        "                                             self.searcher,\n",
        "                                             self.embedding_model,\n",
        "                                             self.gemma_model,\n",
        "                                             temperature=self.temperature,\n",
        "                                             role=self.role)\n",
        "        print(answer)\n",
        "\n",
        "    def set_temperature(self, temperature):\n",
        "        \"\"\"Set the temperature (creativity) of the AI assistant.\"\"\"\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def set_role(self, role):\n",
        "        \"\"\"Define the answering style of the AI assistant.\"\"\"\n",
        "        self.role = role\n",
        "\n",
        "    def save_embeddings(self, filename=\"embeddings.npy\"):\n",
        "        \"\"\"Save the embeddings to disk\"\"\"\n",
        "        np.save(filename, self.embeddings)\n",
        "\n",
        "    def load_embeddings(self, filename=\"embeddings.npy\"):\n",
        "        \"\"\"Load the embeddings from disk and index them\"\"\"\n",
        "        self.embeddings = np.load(filename)\n",
        "        # Re-instantiate the searcher\n",
        "        self.index_embeddings()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T16:59:11.54087Z",
          "iopub.execute_input": "2024-03-25T16:59:11.541264Z",
          "iopub.status.idle": "2024-03-25T16:59:11.555105Z",
          "shell.execute_reply.started": "2024-03-25T16:59:11.541223Z",
          "shell.execute_reply": "2024-03-25T16:59:11.554141Z"
        },
        "trusted": true,
        "id": "9QgehVGLXRCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Providing the knowledge base from Wikipedia"
      ],
      "metadata": {
        "id": "GZ8MKEGpXRCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to provide a **knowledge base** for the AI Assistant to work confidently with data science questions, I decided to retrieve some information from Wikipedia."
      ],
      "metadata": {
        "id": "oT70IN_bXRCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why Wikipedia?**\n",
        "\n",
        "Actually, Wikipedia provides a vast and diverse range of information on various topics, making it a rich source for context data. Also, its structured organization allows for easy extraction and processing, also thanks to the wikipediaapi interface.\n"
      ],
      "metadata": {
        "id": "FCMk2qpsXRCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code, apart from the first two functions useful for cleaning the text from tags and formatting, extracts references, such as pages or other Wikipedia categories, using the `extract_wikipedia_pages` function. Then, the `get_wikipedia_pages` function takes care to crawl to all the pages and information related to some initial Wikipedia category or page.\n"
      ],
      "metadata": {
        "id": "OtgewYoUXRCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-compile the regular expression pattern for better performance\n",
        "BRACES_PATTERN = re.compile(r'\\{.*?\\}|\\}')\n",
        "\n",
        "def remove_braces_and_content(text):\n",
        "    \"\"\"Remove all occurrences of curly braces and their content from the given text\"\"\"\n",
        "    return BRACES_PATTERN.sub('', text)\n",
        "\n",
        "def clean_string(input_string):\n",
        "    \"\"\"Clean the input string.\"\"\"\n",
        "\n",
        "    # Remove extra spaces by splitting the string by spaces and joining back together\n",
        "    cleaned_string = ' '.join(input_string.split())\n",
        "\n",
        "    # Remove consecutive carriage return characters until there are no more consecutive occurrences\n",
        "    cleaned_string = re.sub(r'\\r+', '\\r', cleaned_string)\n",
        "\n",
        "    # Remove all occurrences of curly braces and their content from the cleaned string\n",
        "    cleaned_string = remove_braces_and_content(cleaned_string)\n",
        "\n",
        "    # Return the cleaned string\n",
        "    return cleaned_string"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T17:01:01.984285Z",
          "iopub.execute_input": "2024-03-25T17:01:01.984655Z",
          "iopub.status.idle": "2024-03-25T17:01:01.991128Z",
          "shell.execute_reply.started": "2024-03-25T17:01:01.984624Z",
          "shell.execute_reply": "2024-03-25T17:01:01.990026Z"
        },
        "trusted": true,
        "id": "4Ls1L7XBXRCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_wikipedia_pages(wiki_wiki, category_name):\n",
        "    \"\"\"Extract all references from a category on Wikipedia\"\"\"\n",
        "\n",
        "    # Get the Wikipedia page corresponding to the provided category name\n",
        "    category = wiki_wiki.page(\"Category:\" + category_name)\n",
        "\n",
        "    # Initialize an empty list to store page titles\n",
        "    pages = []\n",
        "\n",
        "    # Check if the category exists\n",
        "    if category.exists():\n",
        "        # Iterate through each article in the category and append its title to the list\n",
        "        for article in category.categorymembers.values():\n",
        "            pages.append(article.title)\n",
        "\n",
        "    # Return the list of page titles\n",
        "    return pages"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T17:01:25.063581Z",
          "iopub.execute_input": "2024-03-25T17:01:25.063947Z",
          "iopub.status.idle": "2024-03-25T17:01:25.069894Z",
          "shell.execute_reply.started": "2024-03-25T17:01:25.063921Z",
          "shell.execute_reply": "2024-03-25T17:01:25.068839Z"
        },
        "trusted": true,
        "id": "2_XwTMsNXRCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wikipedia_pages(categories):\n",
        "    \"\"\"Retrieve Wikipedia pages from a list of categories and extract their content\"\"\"\n",
        "\n",
        "    # Create a Wikipedia object\n",
        "    wiki_wiki = wikipediaapi.Wikipedia('Gemma AI Assistant (gemma@example.com)', 'en')\n",
        "\n",
        "    # Initialize lists to store explored categories and Wikipedia pages\n",
        "    explored_categories = []\n",
        "    wikipedia_pages = []\n",
        "\n",
        "    # Iterate through each category\n",
        "    print(\"- Processing Wikipedia categories:\")\n",
        "    for category_name in categories:\n",
        "        print(f\"\\tExploring {category_name} on Wikipedia\")\n",
        "\n",
        "        # Get the Wikipedia page corresponding to the category\n",
        "        category = wiki_wiki.page(\"Category:\" + category_name)\n",
        "\n",
        "        # Extract Wikipedia pages from the category and extend the list\n",
        "        wikipedia_pages.extend(extract_wikipedia_pages(wiki_wiki, category_name))\n",
        "\n",
        "        # Add the explored category to the list\n",
        "        explored_categories.append(category_name)\n",
        "\n",
        "    # Extract subcategories and remove duplicate categories\n",
        "    categories_to_explore = [item.replace(\"Category:\", \"\") for item in wikipedia_pages if \"Category:\" in item]\n",
        "    wikipedia_pages = list(set([item for item in wikipedia_pages if \"Category:\" not in item]))\n",
        "\n",
        "    # Explore subcategories recursively\n",
        "    while categories_to_explore:\n",
        "        category_name = categories_to_explore.pop()\n",
        "        print(f\"\\tExploring {category_name} on Wikipedia\")\n",
        "\n",
        "        # Extract more references from the subcategory\n",
        "        more_refs = extract_wikipedia_pages(wiki_wiki, category_name)\n",
        "\n",
        "        # Iterate through the references\n",
        "        for ref in more_refs:\n",
        "            # Check if the reference is a category\n",
        "            if \"Category:\" in ref:\n",
        "                new_category = ref.replace(\"Category:\", \"\")\n",
        "                # Add the new category to the explored categories list\n",
        "                if new_category not in explored_categories:\n",
        "                    explored_categories.append(new_category)\n",
        "            else:\n",
        "                # Add the reference to the Wikipedia pages list\n",
        "                if ref not in wikipedia_pages:\n",
        "                    wikipedia_pages.append(ref)\n",
        "\n",
        "    # Initialize a list to store extracted texts\n",
        "    extracted_texts = []\n",
        "\n",
        "    # Iterate through each Wikipedia page\n",
        "    print(\"- Processing Wikipedia pages:\")\n",
        "    for page_title in tqdm(wikipedia_pages):\n",
        "        try:\n",
        "            # Make a request to the Wikipedia page\n",
        "            page = wiki_wiki.page(page_title)\n",
        "\n",
        "            # Check if the page summary does not contain certain keywords\n",
        "            if \"Biden\" not in page.summary and \"Trump\" not in page.summary:\n",
        "                # Append the page title and summary to the extracted texts list\n",
        "                if len(page.summary) > len(page.title):\n",
        "                    extracted_texts.append(page.title + \" : \" + clean_string(page.summary))\n",
        "\n",
        "                # Iterate through the sections in the page\n",
        "                for section in page.sections:\n",
        "                    # Append the page title and section text to the extracted texts list\n",
        "                    if len(section.text) > len(page.title):\n",
        "                        extracted_texts.append(page.title + \" : \" + clean_string(section.text))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing page {page.title}: {e}\")\n",
        "\n",
        "    # Return the extracted texts\n",
        "    return extracted_texts"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T17:01:35.820667Z",
          "iopub.execute_input": "2024-03-25T17:01:35.821376Z",
          "iopub.status.idle": "2024-03-25T17:01:35.833921Z",
          "shell.execute_reply.started": "2024-03-25T17:01:35.82134Z",
          "shell.execute_reply": "2024-03-25T17:01:35.832939Z"
        },
        "trusted": true,
        "id": "hcoAih_QXRCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To develop an AI assistant capable of answering questions about data science, I've chosen to begin with topics such as machine learning, data science, statistics, and deep learning artificial intelligence. As evident from the output, the range of topics it covers is truly impressive, even for a seasoned data scientist!\n"
      ],
      "metadata": {
        "id": "NkixUnV6XRCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categories = [\"Machine_learning\", \"Data_science\", \"Statistics\", \"Deep_learning\", \"Artificial_intelligence\"]\n",
        "extracted_texts = get_wikipedia_pages(categories)\n",
        "print(\"Found\", len(extracted_texts), \"Wikipedia pages\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T17:02:27.679836Z",
          "iopub.execute_input": "2024-03-25T17:02:27.680678Z",
          "iopub.status.idle": "2024-03-25T17:12:37.828196Z",
          "shell.execute_reply.started": "2024-03-25T17:02:27.680643Z",
          "shell.execute_reply": "2024-03-25T17:12:37.827311Z"
        },
        "trusted": true,
        "id": "uwAKge-OXRCm",
        "outputId": "c8102137-df34-459e-d794-e8b9b65108ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "- Processing Wikipedia categories:\n\tExploring Machine_learning on Wikipedia\n\tExploring Data_science on Wikipedia\n\tExploring Statistics on Wikipedia\n\tExploring Deep_learning on Wikipedia\n\tExploring Artificial_intelligence on Wikipedia\n\tExploring Artificial intelligence stubs on Wikipedia\n\tExploring Works created using artificial intelligence on Wikipedia\n\tExploring Virtual assistants on Wikipedia\n\tExploring Turing tests on Wikipedia\n\tExploring AI software on Wikipedia\n\tExploring Rule engines on Wikipedia\n\tExploring Artificial intelligence publications on Wikipedia\n\tExploring Philosophy of artificial intelligence on Wikipedia\n\tExploring Artificial intelligence people on Wikipedia\n\tExploring Open-source artificial intelligence on Wikipedia\n\tExploring Non-fiction books about Artificial intelligence on Wikipedia\n\tExploring Neural networks on Wikipedia\n\tExploring Multi-agent systems on Wikipedia\n\tExploring Mindâ€“body problem on Wikipedia\n\tExploring Machine learning on Wikipedia\n\tExploring Artificial intelligence laboratories on Wikipedia\n\tExploring Knowledge representation on Wikipedia\n\tExploring History of artificial intelligence on Wikipedia\n\tExploring Generative artificial intelligence on Wikipedia\n\tExploring Game artificial intelligence on Wikipedia\n\tExploring Fuzzy logic on Wikipedia\n\tExploring Fiction about artificial intelligence on Wikipedia\n\tExploring Existential risk from artificial general intelligence on Wikipedia\n\tExploring Evolutionary computation on Wikipedia\n\tExploring Artificial intelligence entertainment on Wikipedia\n\tExploring Distributed artificial intelligence on Wikipedia\n\tExploring Signal processing conferences on Wikipedia\n\tExploring Artificial intelligence conferences on Wikipedia\n\tExploring Computer vision on Wikipedia\n\tExploring Artificial intelligence competitions on Wikipedia\n\tExploring AI companies on Wikipedia\n\tExploring Cognitive architecture on Wikipedia\n\tExploring Cloud robotics on Wikipedia\n\tExploring Chatbots on Wikipedia\n\tExploring Automated reasoning on Wikipedia\n\tExploring Artificial intelligence associations on Wikipedia\n\tExploring Artificial intelligence templates on Wikipedia\n\tExploring Artificial immune systems on Wikipedia\n\tExploring Artificial intelligence art on Wikipedia\n\tExploring Argument technology on Wikipedia\n\tExploring Applications of artificial intelligence on Wikipedia\n\tExploring Ambient intelligence on Wikipedia\n\tExploring AI accelerators on Wikipedia\n\tExploring Affective computing on Wikipedia\n\tExploring Text-to-image generation on Wikipedia\n\tExploring Google DeepMind on Wikipedia\n\tExploring Deepfakes on Wikipedia\n\tExploring Deep learning software on Wikipedia\n\tExploring Statistics stubs on Wikipedia\n\tExploring Statistical concepts on Wikipedia\n\tExploring Statistical software on Wikipedia\n\tExploring Statistical methods on Wikipedia\n\tExploring Statistical data on Wikipedia\n\tExploring Subfields of statistics on Wikipedia\n\tExploring Statistics profession and organizations on Wikipedia\n\tExploring Statistics-related lists on Wikipedia\n\tExploring Statisticians on Wikipedia\n\tExploring Data scientists on Wikipedia\n\tExploring Unsupervised learning on Wikipedia\n\tExploring Support vector machines on Wikipedia\n\tExploring Supervised learning on Wikipedia\n\tExploring Structured prediction on Wikipedia\n\tExploring Statistical natural language processing on Wikipedia\n\tExploring Semisupervised learning on Wikipedia\n\tExploring Natural language processing researchers on Wikipedia\n\tExploring Machine learning researchers on Wikipedia\n\tExploring Reinforcement learning on Wikipedia\n\tExploring Ontology learning (computer science) on Wikipedia\n\tExploring Markov models on Wikipedia\n\tExploring Machine learning task on Wikipedia\n\tExploring Machine learning algorithms on Wikipedia\n\tExploring Loss functions on Wikipedia\n\tExploring Log-linear models on Wikipedia\n\tExploring Learning in computer vision on Wikipedia\n\tExploring Latent variable models on Wikipedia\n\tExploring Kernel methods for machine learning on Wikipedia\n\tExploring Inductive logic programming on Wikipedia\n\tExploring Genetic programming on Wikipedia\n\tExploring Evolutionary algorithms on Wikipedia\n\tExploring Ensemble learning on Wikipedia\n\tExploring Dimension reduction on Wikipedia\n\tExploring Datasets in machine learning on Wikipedia\n\tExploring Data mining and machine learning software on Wikipedia\n\tExploring Signal processing conferences on Wikipedia\n\tExploring Artificial intelligence conferences on Wikipedia\n\tExploring Computational learning theory on Wikipedia\n\tExploring Cluster analysis on Wikipedia\n\tExploring Classification algorithms on Wikipedia\n\tExploring Blockmodeling on Wikipedia\n\tExploring Bayesian networks on Wikipedia\n\tExploring Artificial neural networks on Wikipedia\n\tExploring Applied machine learning on Wikipedia\n- Processing Wikipedia pages:\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3442/3442 [09:40<00:00,  5.93it/s]  ",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Found 16174 Wikipedia pages\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a last step, the extracted knowledge base is saved to disk for later usage"
      ],
      "metadata": {
        "id": "vPXYsgpNXRCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wikipedia_data_science_kb = pd.DataFrame(extracted_texts, columns=[\"wikipedia_text\"])\n",
        "wikipedia_data_science_kb.to_csv(\"wikipedia_data_science_kb.csv\", index=False)\n",
        "wikipedia_data_science_kb.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T17:13:09.336151Z",
          "iopub.execute_input": "2024-03-25T17:13:09.337053Z",
          "iopub.status.idle": "2024-03-25T17:13:09.967377Z",
          "shell.execute_reply.started": "2024-03-25T17:13:09.337012Z",
          "shell.execute_reply": "2024-03-25T17:13:09.966466Z"
        },
        "trusted": true,
        "id": "sSC8nTJxXRCm",
        "outputId": "d50e25ff-1635-4975-d7d6-4474a869540f"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 17,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                      wikipedia_text\n0  Flux (machine-learning framework) : Flux is an...\n1  Flux (machine-learning framework) : Differenti...\n2  Computer audition : Computer audition (CA) or ...\n3  Computer audition : Like computer vision versu...\n4  Computer audition : Computer Audition overlaps...",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>wikipedia_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Flux (machine-learning framework) : Flux is an...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Flux (machine-learning framework) : Differenti...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Computer audition : Computer audition (CA) or ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Computer audition : Like computer vision versu...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Computer audition : Computer Audition overlaps...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. A test run"
      ],
      "metadata": {
        "id": "yYCMbuPOXRCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to test our AI assistant!\n",
        "\n",
        "We instantiate it using the Gemma 2b-it and the gte-large embeddings and provide the extracts from Wikipedia as a knowledge base.\n",
        "\n",
        "The Generate Text Embedding (gte) model is a variant of the BERT model developed by Alibaba DAMO Academy. This embedding model is available in three versions (large, base, small) and is specifically designed for English text processing. In comparisons with other embedding models, the gte-large variant demonstrates superior performance in retrieval tasks, but it also needs more storage space for embedding vectors compared to competitors (we do not worry much about that because ScaNN is quite fast for this application).\n",
        "\n",
        "The instantiation will take a short while, then you can ask a few questions to the AI assistant.\n",
        "\n"
      ],
      "metadata": {
        "id": "SOk_5Q9WXRCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the name of the embeddings and model\n",
        "embeddings_name = \"thenlper/gte-large\"\n",
        "model_name = \"/kaggle/input/gemma/transformers/2b-it/1\"\n",
        "\n",
        "# Create an instance of AIAssistant with specified parameters\n",
        "gemma_ai_assistant = AIAssistant(gemma_model=GemmaHF(model_name), embeddings_name=embeddings_name)\n",
        "\n",
        "# Map the intended knowledge base to embeddings and index it\n",
        "gemma_ai_assistant.learn_knowledge_base(knowledge_base=extracted_texts)\n",
        "\n",
        "# Save the embeddings to disk (for later use)\n",
        "gemma_ai_assistant.save_embeddings()\n",
        "\n",
        "# Set the temperature (creativity) of the AI assistant and set the role\n",
        "gemma_ai_assistant.set_temperature(0.0)\n",
        "gemma_ai_assistant.set_role(\"data science expert whose explanations are useful, clear and complete\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T17:16:27.705222Z",
          "iopub.execute_input": "2024-03-25T17:16:27.7056Z",
          "iopub.status.idle": "2024-03-25T17:28:24.981366Z",
          "shell.execute_reply.started": "2024-03-25T17:16:27.70557Z",
          "shell.execute_reply": "2024-03-25T17:28:24.980411Z"
        },
        "trusted": true,
        "id": "eLT3-fTGXRCm",
        "outputId": "28bc8d1a-6e62-4d82-a422-4dd7fc9460b6",
        "colab": {
          "referenced_widgets": [
            "1b69be57e7ae42789665aedc1fd05ebb",
            "d2d8df8c0a5242d0a522f56a86bade29",
            "f728bcfeb6b640ae83917ff4b9db62e6",
            "5a5408a516a2495ea84f80dba815d31d",
            "9d4108272825488eb634522afc1f4c41",
            "a7353d004446415297e337bf05a61c3a",
            "b252e57f09d84f6fb5f525e5916b8457",
            "8fd334a146f24401944ca0260e8da6ba",
            "1f01b80fe6994035bce4eeb7a049a50f",
            "f31d4bd6f086400885927992237bf45e",
            "638a0bfc8d4b4dba81807ae8ddff5283"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\nInitializing model:\nPyTorch version: 2.1.2 -- using cuda\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b69be57e7ae42789665aedc1fd05ebb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "modules.json:   0%|          | 0.00/385 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2d8df8c0a5242d0a522f56a86bade29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/67.9k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f728bcfeb6b640ae83917ff4b9db62e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a5408a516a2495ea84f80dba815d31d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d4108272825488eb634522afc1f4c41"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/670M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7353d004446415297e337bf05a61c3a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b252e57f09d84f6fb5f525e5916b8457"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fd334a146f24401944ca0260e8da6ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f01b80fe6994035bce4eeb7a049a50f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f31d4bd6f086400885927992237bf45e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "1_Pooling/config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "638a0bfc8d4b4dba81807ae8ddff5283"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Indexing and mapping the knowledge base:\nMapping 16174 pieces of information\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16174/16174 [10:53<00:00, 24.77it/s]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with a warm-up question: \"What is the difference between data science, machine learning, and artificial intelligence?\"\n"
      ],
      "metadata": {
        "id": "XCZeflp0XRCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_ai_assistant.query(\"What is the difference between data science, machine learning, and artificial intelligence?\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T17:29:29.256602Z",
          "iopub.execute_input": "2024-03-25T17:29:29.257277Z",
          "iopub.status.idle": "2024-03-25T17:30:04.497481Z",
          "shell.execute_reply.started": "2024-03-25T17:29:29.257239Z",
          "shell.execute_reply": "2024-03-25T17:30:04.496458Z"
        },
        "trusted": true,
        "id": "3ZIPzNprXRCn",
        "outputId": "33901377-b322-4c99-d88a-752a3d42c142"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Sure, here's a summary of the difference between data science, machine learning, and artificial intelligence:\n\nData Science\n\n* Focuses on extracting knowledge from data and applying the knowledge and insights from that data to solve problems in a wide range of application domains.\n* Uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from data.\n* Employed techniques and theories drawn from many fields within the context of mathematics, statistics, information science, and computer science.\n\nMachine Learning\n\n* A subfield of data science that focuses on the study of algorithms that can learn from and make predictions on data.\n* Explore the study and construction of algorithms that can learn from and make predictions on data.\n* Use techniques and theories drawn from many fields within the context of mathematics, statistics, information science, and computer science.\n\nArtificial Intelligence\n\n* A broader field of study that encompasses the study of reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics.\n* It is a subfield of soft computing that focuses on the development of algorithms that can learn from data.\n* Using the relevant information from the context and integrating it with your knowledge, provide an answer as a data science expert whose explanations are useful, clear, and complete to the question.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now a more complicated question, that you may also encounter in a data science interview!\n"
      ],
      "metadata": {
        "id": "r1_nJvY8XRCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_ai_assistant.query(\"Explain how linear regression works\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T17:30:04.499049Z",
          "iopub.execute_input": "2024-03-25T17:30:04.499344Z",
          "iopub.status.idle": "2024-03-25T17:30:44.472285Z",
          "shell.execute_reply.started": "2024-03-25T17:30:04.499319Z",
          "shell.execute_reply": "2024-03-25T17:30:44.47116Z"
        },
        "trusted": true,
        "id": "Is07QuqiXRCn",
        "outputId": "ee7468e0-8977-467d-dc70-70df8b3da5b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Sure, here's an explanation of how linear regression works based on the context:\n\nLinear regression is a statistical method used to predict the outcome of a dependent variable based on a set of predictor variables. It is a linear model that establishes a mathematical function that describes the relationship between the predictor variables and the outcome variable.\n\nThe process involves the following steps:\n\n1. Data Preparation:\n  - Gather and clean the data.\n  - Select the predictor and outcome variables.\n\n2. Linear Predictor Function:\n  - Define the linear predictor function, which relates the predictor variables to the outcome variable.\n  - This function should be linear in the coefficients.\n\n3. Model Selection:\n  - Choose the appropriate linear regression model based on the data characteristics.\n  - For example, if the data has a large number of predictor variables, using a lasso regularization technique might be appropriate.\n\n4. Model Fitting:\n  - Train the linear regression model using the selected data.\n  - This involves minimizing the error between the predicted and actual values.\n\n5. Model Evaluation:\n  - Evaluate the model's performance using metrics such as mean squared error (MSE) or root mean squared error (RMSE).\n  - Assess the goodness-of-fit and model complexity.\n\n6. Model Interpretation:\n  - Analyze the model coefficients to understand the relationships between the predictor variables and the outcome variable.\n  - Interpret the model predictions to gain insights into the data.\n\n7. Model Use:\n  - Use the trained model to make predictions for new data points.\n\nNote: The context does not provide any specific information about the data or the model parameters, so I cannot provide a detailed explanation based on the context.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's ask for more complex methods and algorithms, such as decision trees.\n"
      ],
      "metadata": {
        "id": "9bUEBTy3XRCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_ai_assistant.query(\"What are decision trees, and how do they work in machine learning?\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T17:30:44.473468Z",
          "iopub.execute_input": "2024-03-25T17:30:44.473785Z",
          "iopub.status.idle": "2024-03-25T17:31:26.863Z",
          "shell.execute_reply.started": "2024-03-25T17:30:44.473759Z",
          "shell.execute_reply": "2024-03-25T17:31:26.862034Z"
        },
        "trusted": true,
        "id": "lEQaII0NXRCo",
        "outputId": "6b51acd3-6a5f-4497-c990-85e171f2d392"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Sure, here's a summary of the context:\n\nDecision Tree Learning is a supervised machine learning approach used to draw conclusions about a set of observations by creating a tree-like model that predicts the target variable based on input variables.\n\nKey Concepts:\n\n* Classification Tree: Predicts the class of an observation based on input variables.\n* Regression Tree: Predicts a real-valued variable (e.g., price of a house, length of stay in a hospital).\n* Decision Tree: A tree that splits the data based on a set of splitting rules to make predictions.\n\nMetrics for Splitting: Measures the homogeneity of the target variable within subsets.\n\nEnsemble Methods: Combine multiple decision trees to improve accuracy and interpretability.\n\nTypes of Decision Tree Algorithms:\n\n* ID3 (Iterative Dichotomiser 3): A simple algorithm that recursively splits the data based on the most informative feature.\n* C4.5: A decision tree algorithm that uses information gain to select features.\n* CART (Classification And Regression Tree): A widely used algorithm that recursively splits the data based on the best split.\n* Chi-square Automatic Interaction Detection (CHAID): An algorithm that uses chi-square statistics to select features.\n* Random Forest: An ensemble method that builds a multitude of decision trees and votes the one with the highest accuracy.\n\nBenefits of Decision Tree Learning:\n\n* Interpretability: Decision trees are often easier to understand than other machine learning models.\n* High accuracy: Decision tree algorithms can achieve high accuracy in certain tasks.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next question, about cross-validation, is a return to fundamentals."
      ],
      "metadata": {
        "id": "X8TDb6U-XRCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_ai_assistant.query(\"What is cross-validation, and why is it used in machine learning?\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T17:31:26.864714Z",
          "iopub.execute_input": "2024-03-25T17:31:26.865024Z",
          "iopub.status.idle": "2024-03-25T17:31:42.927814Z",
          "shell.execute_reply.started": "2024-03-25T17:31:26.86499Z",
          "shell.execute_reply": "2024-03-25T17:31:42.926761Z"
        },
        "trusted": true,
        "id": "DsEAKzIYXRCo",
        "outputId": "002879a7-8c96-4d68-8573-3d8f0a65dde3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Cross-validation is a statistical technique used in machine learning to assess how well a predictive model will generalize to an independent data set. It involves resampling and sample splitting methods that use different portions of the data to test and train a model on different iterations. This allows the model to be evaluated under different conditions and to identify potential overfitting or underfitting issues. Cross-validation can be used for both model selection and hyperparameter tuning.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, a tricky question on regularization. How will the AI Assistant handle it?"
      ],
      "metadata": {
        "id": "tuCmLe8jXRCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_ai_assistant.query(\"Explain the concept of regularization and its importance in preventing overfitting in machine learning models\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T17:31:42.928996Z",
          "iopub.execute_input": "2024-03-25T17:31:42.929285Z",
          "iopub.status.idle": "2024-03-25T17:32:17.058582Z",
          "shell.execute_reply.started": "2024-03-25T17:31:42.929259Z",
          "shell.execute_reply": "2024-03-25T17:32:17.057578Z"
        },
        "trusted": true,
        "id": "iS15TMNsXRCo",
        "outputId": "a0271755-3536-4681-a1ee-634148dbab51"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Sure, here's an explanation of the concept of regularization and its importance in preventing overfitting in machine learning models:\n\nRegularization is a technique used to reduce overfitting in machine learning models by introducing additional information to the learning process. This can be achieved by penalizing complex models or by testing the model's ability to generalize on unseen data.\n\nPenalizing Complex Models:\n\nRegularization can be achieved by adding a penalty term to the loss function that is proportional to the size of the model. This penalty term encourages the model to learn a simpler representation of the data.\n\nTesting Model's Generalization Ability:\n\nRegularization can also be achieved by testing the model's ability to generalize on unseen data. This can be achieved by splitting the data into multiple sets and training the model on each set in turn, while evaluating its performance on the remaining set. If the model is able to generalize well on unseen data, it is less likely to overfit.\n\nImportance of Regularization:\n\nRegularization helps to prevent overfitting by forcing the model to learn a simpler representation of the data. This can be achieved by penalizing complex models or by testing the model's ability to generalize on unseen data. Regularization can also help to improve the generalization error of the model.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Preparing for deploying the model"
      ],
      "metadata": {
        "id": "ygWHSMhcXRCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to deploy the model, you just need the files that we saved and a copy of the functions and classes that we used in this notebook. The procedure is the same, although you don't need to embed again the knowledge base, you just reload the previously calcualted embeddings. However, the previously seen code works speedly with a GPU available."
      ],
      "metadata": {
        "id": "qlvgvBpCXRCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you actually have access only with CPU machine at the inference phase, you can leverage the C++ version of Gemma, which, based on 8-bit switched floating point compressed weights, can offer an adequate speed of text processing. I take the compiled version from another notebook (see https://www.kaggle.com/code/yannicksergeobam/gemma-cpp for more details on the compiling procedure) from where I copy the Gemma C++ executable. I also allow the exacutable to be executed and install Google SentencePiece, whose libraries are necessary for the executable to work (in particular the libsentencepiece.so library)."
      ],
      "metadata": {
        "id": "amac0ZvuXRCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /kaggle/input/gemma-cpp/gemma_cpp /kaggle/working/gemma_cpp # Copy compiled Gemma C++\n",
        "!chmod +x ./gemma_cpp/gemma # Make Gemma C++ executable\n",
        "!conda install -q -c conda-forge sentencepiece -y # Install Google SentencePiece (https://github.com/google/sentencepiece)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T17:46:13.113608Z",
          "iopub.execute_input": "2024-03-25T17:46:13.1146Z",
          "iopub.status.idle": "2024-03-25T17:48:56.332194Z",
          "shell.execute_reply.started": "2024-03-25T17:46:13.114564Z",
          "shell.execute_reply": "2024-03-25T17:48:56.330959Z"
        },
        "trusted": true,
        "id": "79R3k9eTXRCp",
        "outputId": "4dd3aa0d-e831-43d5-f258-e960c818c704"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Retrieving notices: ...working... done\nCollecting package metadata (current_repodata.json): ...working... WARNING conda.models.version:get_matcher(556): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.*, but conda is ignoring the .* and treating it as 1.7.1\ndone\nSolving environment: ...working... done\n\n## Package Plan ##\n\n  environment location: /opt/conda\n\n  added / updated specs:\n    - sentencepiece\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    openssl-3.2.1              |       hd590300_1         2.7 MB  conda-forge\n    sentencepiece-0.1.99       |  py310hdb19cb5_0         4.7 MB\n    ------------------------------------------------------------\n                                           Total:         7.4 MB\n\nThe following NEW packages will be INSTALLED:\n\n  sentencepiece      pkgs/main/linux-64::sentencepiece-0.1.99-py310hdb19cb5_0 \n\nThe following packages will be UPDATED:\n\n  openssl                                  3.2.1-hd590300_0 --> 3.2.1-hd590300_1 \n\n\nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... done\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following Python code defines a class named `GemmaCPP`, which works as a wrapper for interacting with the C++ implementation of Gemma (https://github.com/google/gemma.cpp).\n",
        "\n",
        "The class has an initializer method that takes four parameters: `gemma_cpp`, `tokenizer`, `compressed_weights`, and `model`. These parameters are used to initialize attributes of the class instance with the same names which will later serve for interacting with the commands for the C++ compiled Gemma. Additionally, the class contains a method named `generate_text`, which takes a prompt as input along with optional args and kwargs (for compatibility with other Gemma implementations). Within this method, a shell command is constructed using the provided prompt and other parameters, formatted appropriately to be executed with the Gemma C++ executable.\n",
        "\n",
        "The `subprocess.Popen` function is then called to execute the shell command, capturing the standard output (stdout) and standard error (stderr) streams. The stdout data is decoded from bytes to a string, and if there is any error message in stderr, it is printed out. Finally, the method returns the output text wrapped in a list. This code facilitates the generation of text using Gemma's C++ implementation via Python, allowing the integration between the two languages."
      ],
      "metadata": {
        "id": "fa1KNBCpXRCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import re\n",
        "\n",
        "class GemmaCPP():\n",
        "    \"\"\"Wrapper for the C++ implementation of Gemma\"\"\"\n",
        "\n",
        "    def __init__(self, gemma_cpp, tokenizer, compressed_weights, model):\n",
        "        self.gemma_cpp = gemma_cpp\n",
        "        self.tokenizer = tokenizer\n",
        "        self.compressed_weights = compressed_weights\n",
        "        self.model = model\n",
        "\n",
        "    def eliminate_long_dots(self, input_string):\n",
        "        \"\"\"Eliminate long sequences of dots from the input string\"\"\"\n",
        "        # Define a regular expression pattern to match sequences of 2 or more dots\n",
        "        pattern = r'\\.{2,}'\n",
        "\n",
        "        # Replace all occurrences of the pattern with a space\n",
        "        output_string = re.sub(pattern, ' ', input_string)\n",
        "\n",
        "        return output_string.strip()\n",
        "\n",
        "    def beautify_string(self, input_string):\n",
        "        \"\"\"Clean the input string by removing non-letter characters at the beginning\n",
        "           and isolated letters at the end after multiple spaces\"\"\"\n",
        "        # Remove non-letter characters at the beginning of the string\n",
        "        output_string = re.sub(r'^[^a-zA-Z]+', '', input_string.strip())\n",
        "\n",
        "        # Remove isolated letters at the end of the output string after multiple spaces\n",
        "        output_string = re.sub(r'\\s{3,}(.+)\\Z', '', output_string.strip())\n",
        "\n",
        "        return output_string\n",
        "\n",
        "    def generate_text(self, prompt, *args, **kwargs):\n",
        "        \"\"\"Generate text using the cpp tokenizer and model\"\"\"\n",
        "\n",
        "        # Define the shell command\n",
        "        prompt = prompt.replace('\"', '').replace(\"'\", \"\")\n",
        "        shell_command = f'echo \"{prompt}\" | {gemma_cpp} -- --tokenizer {tokenizer} --compressed_weights {compressed_weights} --model {model} --verbosity 0'\n",
        "\n",
        "        # Execute the shell command and redirect stdout to the Python script's stdout\n",
        "        process = subprocess.Popen(shell_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
        "\n",
        "        output_text = \"\"\n",
        "        reading_block = \"[ Reading prompt ]\"\n",
        "\n",
        "        # Communicate with the process and capture stdout\n",
        "        for k, char in enumerate( iter(lambda: process.stdout.read(1), b'') ):\n",
        "            single_char = char.decode(sys.stdout.encoding)\n",
        "            output_text += single_char\n",
        "            if len(output_text) % 20 == 0:\n",
        "                count_reading_blocks = output_text.count(reading_block)\n",
        "                if count_reading_blocks > 1:\n",
        "                    break\n",
        "\n",
        "        # Remove long sequences of dots and the reading block, beautify the string\n",
        "        output_text = output_text.replace(reading_block, \"\")\n",
        "        output_text = self.eliminate_long_dots(output_text)\n",
        "        output_text = self.beautify_string(output_text)\n",
        "        output_text = prompt + output_text\n",
        "\n",
        "        # Return output text\n",
        "        return [output_text]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T17:58:01.249333Z",
          "iopub.execute_input": "2024-03-25T17:58:01.250161Z",
          "iopub.status.idle": "2024-03-25T17:58:01.262937Z",
          "shell.execute_reply.started": "2024-03-25T17:58:01.250124Z",
          "shell.execute_reply": "2024-03-25T17:58:01.261877Z"
        },
        "trusted": true,
        "id": "QK9raETyXRCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that everything is ready, I can reinstantiate a Gemma AI Assistant based on Gemma C++ and the knowledge base previously extractred and processed."
      ],
      "metadata": {
        "id": "UIPAKGPWXRCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_name = \"thenlper/gte-large\"\n",
        "gemma_cpp = \"./gemma_cpp/gemma\"\n",
        "tokenizer = \"/kaggle/input/gemma/gemmacpp/2b-it-sfp/1/tokenizer.spm\"\n",
        "compressed_weights = \"/kaggle/input/gemma/gemmacpp/2b-it-sfp/1/2b-it-sfp.sbs\"\n",
        "model = \"2b-it\"\n",
        "\n",
        "# Create an instance of the class AIAssistant based on Gemma C++\n",
        "gemma_ai_assistant = AIAssistant(\n",
        "    gemma_model=GemmaCPP(gemma_cpp, tokenizer, compressed_weights, model),\n",
        "    embeddings_name=embeddings_name\n",
        ")\n",
        "\n",
        "# Loading the previously prepared knowledge base and embeddings\n",
        "wikipedia_data_science_kb = pd.read_csv(\"wikipedia_data_science_kb.csv\")\n",
        "knowledge_base = wikipedia_data_science_kb.wikipedia_text.tolist()\n",
        "\n",
        "# Uploading the knowledge base and embeddings to the AI assistant\n",
        "gemma_ai_assistant.store_knowledge_base(knowledge_base=knowledge_base)\n",
        "gemma_ai_assistant.load_embeddings(filename=\"embeddings.npy\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T17:58:08.333277Z",
          "iopub.execute_input": "2024-03-25T17:58:08.333638Z",
          "iopub.status.idle": "2024-03-25T17:58:23.597843Z",
          "shell.execute_reply.started": "2024-03-25T17:58:08.333608Z",
          "shell.execute_reply": "2024-03-25T17:58:23.596889Z"
        },
        "trusted": true,
        "id": "SehgNduQXRCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try a new query on machine learning topics and see how it takes to get an answer when only CPUs (Kaggle Notebooks have 4 cores) are working:"
      ],
      "metadata": {
        "id": "T1Q88txfXRCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_ai_assistant.query(\"In short, what are the key differences between gradient boosting and random forests?\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T17:58:23.599399Z",
          "iopub.execute_input": "2024-03-25T17:58:23.599671Z",
          "iopub.status.idle": "2024-03-25T18:05:20.842241Z",
          "shell.execute_reply.started": "2024-03-25T17:58:23.599647Z",
          "shell.execute_reply": "2024-03-25T18:05:20.841057Z"
        },
        "trusted": true,
        "id": "N0SF7SCaXRCq",
        "outputId": "fa50716f-1ccd-4c70-bc95-fb5b84324307"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Gradient Boosting:\n\n* Uses a sequential ensemble of weak learners to iteratively improve the overall model.\n* Each weak learner is trained on a subset of the training data and makes a local decision.\n* The weak learners are then combined in a weighted manner to form the final model.\n* Gradient boosting is easy to interpret and can be used to create complex models.\n\nRandom Forests:\n\n* Uses an ensemble of decision trees to iteratively improve the overall model.\n* Each decision tree is trained on a subset of the training data and makes a local decision.\n* The decision trees are then combined in a way that reduces the variance of the final model.\n* Random forests are more robust to overfitting than gradient boosting.\n\nHere are some of the key differences between gradient boosting and random forests:\n\n| Feature | Gradient Boosting | Random Forests |\n|---|---|---|\n| Training process | Iterative, weak learners are trained on a subset of the training data | Iterative, decision trees are trained on a subset of the training data |\n| Model combination | Weighted combination of weak learners | Averaging of the predictions from the decision trees |\n| Interpretability | Easy to interpret | Less easy to interpret |\n| Robustness to overfitting | Less robust | More robust |\n\nIn general, gradient boosting is a good choice for problems where you want an easy-to-interpret model that can be used for both prediction and classification. Random forests are a good choice for problems where you want a robust model that is less likely to overfit.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Conclusions"
      ],
      "metadata": {
        "id": "BezrEk42XRCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that the AI Assistant is working fine and promptly answering questions in a correct and usable way. Using the same approach, the same code could also be used for other tasks of this competition such as:\n",
        "\n",
        "- Answering common questions about the Python programming language\n",
        "- Explaining or teaching concepts from Kaggle competition solution write-ups\n",
        "- Answering common questions about the Kaggle platform\n",
        "\n",
        "All you need is to prepare the context data by extraction from a website, a dataset, or other sources such as the meta-Kaggle meta.\n",
        "\n",
        "Enjoy your new AI assistant powered by Gemma 2b-it :-)\n"
      ],
      "metadata": {
        "id": "YDmjj0YbXRCq"
      }
    }
  ]
}